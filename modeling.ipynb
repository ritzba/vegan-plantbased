{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6952005-e7e1-4cc4-b2c1-a9d80513cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45a391f8-c625-40d9-8c37-c18cfa7324ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "973447be-f236-4d75-96a8-e11b3442be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "23fe8fd6-61a7-4c63-9b84-b7dcda1235ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5a85c7f0-a1f6-42a3-9576-f13d3fe8425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e46fe799-48ef-48f3-8224-e67c2b024477",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "20668b91-3e04-41ee-91cc-b5cd9f0ba96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c4a79771-3607-4a6c-adee-092bae9a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e5342ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cec1cfb5-9f99-4140-9308-f71855862b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "427daf9f-1b62-4154-8b86-e31ee7174eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6b318e2b-dd5c-4673-8375-7c4fd740ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ccb7c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c301a3ea-a03d-4a83-8549-483858731472",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9b9a7e-b1b5-478c-acdb-c21d7a085b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0c13d611-a744-496d-abc5-35f8169bac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = posts['selftext']\n",
    "y = posts['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f2c5e-7808-4928-83ca-fb1be49b3a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f4e89a8f-70c3-4f95-b5b3-739932790b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify = y, \n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "07dc9a11-215a-430d-9656-4ce221b53a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18522,), (18522,), (6175,), (6175,))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9b0b63f9-8733-4110-aede-fabe68d5f41e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.693284\n",
       "1    0.306716\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef2b3254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a205b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = [re.sub('http\\S+','',post) for post in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5888ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e58dbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class spacy_tokenizer():\n",
    "    def __init__(self):\n",
    "        self\n",
    "    def __call__(self,post):\n",
    "        doc = nlp(post)\n",
    "        return [token.lemma_ for token in doc if \n",
    "                         token.pos_.lower() not in ['aux','punct','cconj','det','space','conj','adp','pron','sym']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf83e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "40828441",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words = 'english', \n",
    "                       max_features = 1000, \n",
    "                       min_df = 2,\n",
    "                       max_df = .9,\n",
    "                       ngram_range = (1,1),\n",
    "                       tokenizer = spacy_tokenizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8cfbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "e31e322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train = tvec.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e56d9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "21601594",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train.toarray(), columns = tvec.get_feature_names_out())\n",
    "X_test = pd.DataFrame(X_test.toarray(), columns = tvec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622dfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9b2025a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vegan       1249.598444\n",
       "eat          814.577687\n",
       "just         706.280772\n",
       "make         645.689529\n",
       "animal       636.186979\n",
       "food         586.315495\n",
       "know         582.356413\n",
       "want         525.673672\n",
       "meat         489.845338\n",
       "diet         487.714444\n",
       "good         481.061734\n",
       "think        474.931323\n",
       "try          466.601340\n",
       "feel         454.399555\n",
       "people       449.176400\n",
       "iâm        447.057466\n",
       "plant        444.105976\n",
       "use          442.698422\n",
       "like         431.671991\n",
       "really       429.907858\n",
       "base         422.402375\n",
       "say          402.781819\n",
       "thank        387.616150\n",
       "look         383.148770\n",
       "time         382.283366\n",
       "year         373.469761\n",
       "day          372.818154\n",
       "thing        353.280142\n",
       "recipe       346.606304\n",
       "[            334.038499\n",
       "help         329.885639\n",
       "product      327.226724\n",
       "need         325.678891\n",
       "love         313.379298\n",
       "lot          310.016422\n",
       "iâve       303.006584\n",
       "start        301.346978\n",
       "way          300.848584\n",
       "milk         295.891218\n",
       "meal         295.043665\n",
       "buy          285.948762\n",
       "itâs       265.056779\n",
       "work         261.816329\n",
       "cook         247.861992\n",
       "come         242.310688\n",
       "week         240.723342\n",
       "wonder       240.600787\n",
       "veganism     235.478704\n",
       "protein      235.131780\n",
       "idea         233.855743\n",
       "dtype: float64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.sum().sort_values(ascending = False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e009e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8d40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49f312be-fb2a-40b2-a994-4936160111ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dict = {'mnb': # 1st level 1st key\n",
    "              {'model': # 2nd level 1st key\n",
    "               ('mnb', MultinomialNB()),\n",
    "               'params':{}\n",
    "              },\n",
    "  #            'knn': # 1st level 2nd key\n",
    "   #           {'model': # 2nd level 1st key\n",
    "    #           ('knn', KNeighborsClassifier()),\n",
    "     #          'params':# 2nd level 2nd key\n",
    "      #         {\"knn__n_neighbors\": [3,5,7,10],\n",
    "       #         \"knn__p\":[1,2,3]}},\n",
    "             'logr': # 1st level 2nd key\n",
    "              {'model': # 2nd level 1st key\n",
    "               ('logr', LogisticRegression()),\n",
    "               'params':# 2nd level 2nd key\n",
    "               {\"logr__C\": [1,10,100]}},\n",
    "#             'abc': # 1st level 2nd key\n",
    " #             {'model': # 2nd level 1st key\n",
    "  #             ('abc', AdaBoostClassifier()),\n",
    "   #            'params':# 2nd level 2nd key\n",
    "    #           {\"abc__n_estimators\": [30,50,100],\n",
    "     #           \"abc__learning_rate\": [0.2,0.4,0.6]}}\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ec3e46a-eb75-440b-829c-7d0231bf2175",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28940\\2082903818.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbest_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# for loop through the 3 1st level keys of model_model dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;31m# set pipeline with Count Vectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     pipe = Pipeline([('tvec', TfidfVectorizer(stop_words = stopwords, \n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# set dictionary to house best estimators and params\n",
    "best_models = {}\n",
    "# for loop through the 3 1st level keys of model_model dict\n",
    "for key, value in model_dict.items():\n",
    "    # set pipeline with Count Vectorizer\n",
    "    pipe = Pipeline([('tvec', TfidfVectorizer(stop_words = stopwords, \n",
    "                                              max_features = 1000, \n",
    "                                              min_df = 2,\n",
    "                                              max_df = .9,\n",
    "                                              ngram_range = (1,1),\n",
    "                                              tokenizer = LemmaTokenizer()))])\n",
    "    # append current model iteration tuple\n",
    "    pipe.steps.append(value['model'])\n",
    "    print(pipe)\n",
    "    gs = GridSearchCV(pipe, # set as iteration of pipe\n",
    "                  value['params'], # set as iteration of pipe_params\n",
    "                      cv=5, # Cross Validation of 5\n",
    "                      n_jobs=-1) # Unlock CPU to help process\n",
    "    # fit the current iteration of GridSearchCV\n",
    "    gs.fit(X_train, y_train)\n",
    "    # save the best model, best params, and the pipe for scoring \n",
    "    best_models[key] = {'model': gs.best_estimator_,\n",
    "                        'params': gs.best_params_,\n",
    "                        'pipe': pipe}\n",
    "    print(best_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6f0ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in best_models.items():\n",
    "    value['model'].fit(X_train,y_train)\n",
    "    print(value['model'].score(X_train, y_train), value['model'].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "723e0051-c5bf-46fe-84d4-d47db7e7bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce9f5b-5cad-4b53-a19f-5e085dc6c8a9",
   "metadata": {},
   "source": [
    "# LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "6c3fc41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\numpy\\core\\function_base.py:277: RuntimeWarning: overflow encountered in power\n",
      "  return _nx.power(base, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0023052380778996)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_logr = GridSearchCV(LogisticRegression(),\n",
    "                             param_grid = {'C':np.logspace(0.001,1000,num=10)},\n",
    "                             cv=5,                \n",
    "                             verbose=1,                  \n",
    "                             n_jobs = -1)\n",
    "gs_logr.fit(X_train, y_train)\n",
    "logr = gs_logr.best_estimator_\n",
    "logr.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6ca4b509",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2251852093.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Finley & Stein\\AppData\\Local\\Temp\\ipykernel_13508\\2251852093.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    (k,v) for k,v in gs_logr.best_params_.items(),\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "scores.append(['logr',\n",
    "               [(k,v) for k,v in gs_logr.best_params_.items()],\n",
    "               logr.score(X_train, y_train),\n",
    "               logr.score(X_test, y_test),\n",
    "               cross_val_score(logr,X_test,y_test,cv=5).mean()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "267fddf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['logr',\n",
       "  [('C', 1.0023052380778996)],\n",
       "  0.8874851527912753,\n",
       "  0.8723886639676114,\n",
       "  0.8618623481781376]]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "c723ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(logr.predict(X_test)).to_csv('./preds/logr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5573174-ad23-491a-9e50-32c3652c50a6",
   "metadata": {},
   "source": [
    "#  MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "96399fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gs_mnb = GridSearchCV(MultinomialNB(),\n",
    "                  param_grid = {'alpha':np.linspace(0,1,num=10)},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs_mnb.fit(X_train, y_train)\n",
    "mnb = gs_mnb.best_estimator_\n",
    "mnb.fit(X_train,y_train)\n",
    "scores.append(['mnb',\n",
    "               [(k,v) for k,v in gs_mnb.best_params_.items()],\n",
    "               mnb.score(X_train, y_train),\n",
    "               mnb.score(X_test, y_test),\n",
    "               cross_val_score(mnb,X_test,y_test,cv=5).mean()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b84c478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mnb.predict(X_test)).to_csv('./preds/mnb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "eeb3afa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['logr',\n",
       "  [('C', 1.0023052380778996)],\n",
       "  0.8874851527912753,\n",
       "  0.8723886639676114,\n",
       "  0.8618623481781376],\n",
       " ['mnb',\n",
       "  [('alpha', 0.0)],\n",
       "  0.8325234855847101,\n",
       "  0.825748987854251,\n",
       "  0.8242914979757086]]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9cc680-2aed-4abb-942e-b72300d862dd",
   "metadata": {},
   "source": [
    "# Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "03647541-aaf8-4c65-8bb9-000d33e45aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.0)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_gnb = GridSearchCV(GaussianNB(),\n",
    "                  param_grid = {'var_smoothing':[1e-7,1e-8,1e-9,1e-10]},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs_gnb.fit(X_train, y_train)\n",
    "gnb = gs.best_estimator_\n",
    "gnb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cfc59766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py:555: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scores.append(['gnb',\n",
    "               [(k,v) for k,v in gs_gnb.best_params_.items()],\n",
    "               gnb.score(X_train, y_train),\n",
    "               gnb.score(X_test, y_test),\n",
    "               cross_val_score(mnb,X_test,y_test,cv=5).mean()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "789abf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gnb.predict(X_test)).to_csv('./preds/gnb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0821587f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['logr',\n",
       "  [('C', 1.0023052380778996)],\n",
       "  0.8874851527912753,\n",
       "  0.8723886639676114,\n",
       "  0.8618623481781376],\n",
       " ['mnb',\n",
       "  [('alpha', 0.0)],\n",
       "  0.8325234855847101,\n",
       "  0.825748987854251,\n",
       "  0.8242914979757086],\n",
       " ['gnb',\n",
       "  [('var_smoothing', 1e-07)],\n",
       "  0.8325234855847101,\n",
       "  0.825748987854251,\n",
       "  0.8242914979757086]]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a207c4-7e47-4899-8051-c72b736fe0c8",
   "metadata": {},
   "source": [
    "# Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf0493c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72e405ea-772b-4628-9935-2f10bfc3ead0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "67260cce-9fc2-42d9-952b-399fce7bef76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(KNeighborsClassifier(),\n",
    "                  param_grid = {'n_neighbors':[3,5,7,10,15]},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "knn = gs.best_estimator_\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d9096de2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13508\\1456308191.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m scores.append(['knn',\n\u001b[1;32m----> 2\u001b[1;33m                \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m                \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                cross_val_score(knn,X_test,y_test,cv=5).mean()]) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs_knn' is not defined"
     ]
    }
   ],
   "source": [
    "scores.append(['knn',\n",
    "               [(k,v) for k,v in gs_knn.best_params_.items()],\n",
    "               knn.score(X_train, y_train),\n",
    "               knn.score(X_test, y_test),\n",
    "               cross_val_score(knn,X_test,y_test,cv=5).mean()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b712b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(knn.predict(X_test)).to_csv('./preds/gnb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bea505",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f64f04-adcb-4178-a0e8-85b567fbd436",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e968e088",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.831497678436454, 0.8297975708502024)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(DecisionTreeClassifier(),\n",
    "                  param_grid = {'max_depth':[3,5,7]},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "dt = gs.best_estimator_\n",
    "dt.fit(X_train,y_train)\n",
    "dt.score(X_train, y_train), dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e855f-6eb1-4912-a279-6fc36729f6cc",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "23141f7b-78de-4f47-a7c0-95b792b05efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.77286470143613, 0.7674493927125506)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(RandomForestClassifier(),\n",
    "                  param_grid = {\n",
    "                      'n_estimators':[30,50,100],\n",
    "                      'max_depth':[3,5,7]},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "rf = gs.best_estimator_\n",
    "rf.fit(X_train,y_train)\n",
    "rf.score(X_train, y_train), rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b80fe-10dd-4a40-ac9a-94c3a8b306a1",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c41c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(AdaBoostClassifier(),\n",
    "                  param_grid = {\n",
    "                      'n_estimators':[30,50,100,200]},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "abc = gs.best_estimator_\n",
    "abc.fit(X_train,y_train)\n",
    "abc.score(X_train, y_train), abc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26671f3-7456-42c1-90bb-7629755593f5",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f465918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d73c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dd61a44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '--',\n",
       " '.',\n",
       " '..',\n",
       " '...',\n",
       " '....',\n",
       " '/',\n",
       " '//www.youtube.com/watch',\n",
       " '1',\n",
       " '1.',\n",
       " '1/2',\n",
       " '10',\n",
       " '100',\n",
       " '15',\n",
       " '2',\n",
       " '2.',\n",
       " '20',\n",
       " '3',\n",
       " '3.',\n",
       " '30',\n",
       " '4',\n",
       " '5',\n",
       " '50',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " '@',\n",
       " '_',\n",
       " '\\\\',\n",
       " '\\\\-',\n",
       " '_',\n",
       " '``',\n",
       " 'able',\n",
       " 'absolutely',\n",
       " 'abuse',\n",
       " 'according',\n",
       " 'acid',\n",
       " 'action',\n",
       " 'active',\n",
       " 'activism',\n",
       " 'activist',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'advance',\n",
       " 'advice',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'agriculture',\n",
       " 'allergic',\n",
       " 'allergy',\n",
       " 'allowed',\n",
       " 'almond',\n",
       " 'alternative',\n",
       " 'amazing',\n",
       " 'american',\n",
       " 'amp',\n",
       " 'animal',\n",
       " 'answer',\n",
       " 'anxiety',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anyways',\n",
       " 'app',\n",
       " 'apparently',\n",
       " 'apple',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'approach',\n",
       " 'area',\n",
       " 'arenâ\\x80\\x99t',\n",
       " 'argument',\n",
       " 'article',\n",
       " 'ask',\n",
       " 'asked',\n",
       " 'asking',\n",
       " 'assume',\n",
       " 'ate',\n",
       " 'auto=webp',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avocado',\n",
       " 'avoid',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'b12',\n",
       " 'baby',\n",
       " 'bacon',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'baking',\n",
       " 'banana',\n",
       " 'bar',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'bean',\n",
       " 'bee',\n",
       " 'beef',\n",
       " 'belief',\n",
       " 'believe',\n",
       " 'benefit',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'biggest',\n",
       " 'bit',\n",
       " 'black',\n",
       " 'blood',\n",
       " 'body',\n",
       " 'book',\n",
       " 'bought',\n",
       " 'bowl',\n",
       " 'boyfriend',\n",
       " 'brain',\n",
       " 'brand',\n",
       " 'bread',\n",
       " 'break',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'broccoli',\n",
       " 'brother',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'bunch',\n",
       " 'burger',\n",
       " 'business',\n",
       " 'butter',\n",
       " 'buy',\n",
       " 'buying',\n",
       " 'cake',\n",
       " 'calcium',\n",
       " 'called',\n",
       " 'calorie',\n",
       " 'came',\n",
       " 'cancer',\n",
       " 'cant',\n",
       " 'canâ\\x80\\x99t',\n",
       " 'car',\n",
       " 'carb',\n",
       " 'carbs',\n",
       " 'care',\n",
       " 'carrot',\n",
       " 'case',\n",
       " 'cashew',\n",
       " 'cat',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'cereal',\n",
       " 'certain',\n",
       " 'challenge',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'changing',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'cheese',\n",
       " 'chicken',\n",
       " 'chickpea',\n",
       " 'child',\n",
       " 'chip',\n",
       " 'chocolate',\n",
       " 'choice',\n",
       " 'cholesterol',\n",
       " 'choose',\n",
       " 'city',\n",
       " 'claim',\n",
       " 'clean',\n",
       " 'clear',\n",
       " 'click',\n",
       " 'close',\n",
       " 'coconut',\n",
       " 'coffee',\n",
       " 'cold',\n",
       " 'college',\n",
       " 'come',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'common',\n",
       " 'community',\n",
       " 'company',\n",
       " 'compared',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'condition',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'constantly',\n",
       " 'consume',\n",
       " 'consuming',\n",
       " 'consumption',\n",
       " 'contain',\n",
       " 'contains',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'control',\n",
       " 'conversation',\n",
       " 'convince',\n",
       " 'cook',\n",
       " 'cookbook',\n",
       " 'cooked',\n",
       " 'cooking',\n",
       " 'cool',\n",
       " 'corn',\n",
       " 'cost',\n",
       " 'couldnâ\\x80\\x99t',\n",
       " 'country',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cow',\n",
       " 'craving',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'crop',\n",
       " 'cruelty',\n",
       " 'culture',\n",
       " 'cup',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'curry',\n",
       " 'cut',\n",
       " 'cutting',\n",
       " 'd',\n",
       " 'dad',\n",
       " 'daily',\n",
       " 'dairy',\n",
       " 'data',\n",
       " 'date',\n",
       " 'day',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'debate',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'deficiency',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'despite',\n",
       " 'diabetes',\n",
       " 'didnâ\\x80\\x99t',\n",
       " 'die',\n",
       " 'diet',\n",
       " 'dietary',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'discussion',\n",
       " 'disease',\n",
       " 'dish',\n",
       " 'doctor',\n",
       " 'documentary',\n",
       " 'doe',\n",
       " 'doesnâ\\x80\\x99t',\n",
       " 'dog',\n",
       " 'dominion',\n",
       " 'dont',\n",
       " 'donâ\\x80\\x99t',\n",
       " 'dr',\n",
       " 'dr.',\n",
       " 'dressing',\n",
       " 'dried',\n",
       " 'drink',\n",
       " 'drinking',\n",
       " 'dry',\n",
       " 'early',\n",
       " 'easier',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'eaten',\n",
       " 'eater',\n",
       " 'eating',\n",
       " 'eats',\n",
       " 'edit',\n",
       " 'effect',\n",
       " 'effort',\n",
       " 'egg',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'energy',\n",
       " 'enjoy',\n",
       " 'entire',\n",
       " 'entirely',\n",
       " 'environment',\n",
       " 'environmental',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'ethic',\n",
       " 'ethical',\n",
       " 'event',\n",
       " 'eventually',\n",
       " 'everyday',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'excited',\n",
       " 'exercise',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'experienced',\n",
       " 'explain',\n",
       " 'exploitation',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'face',\n",
       " 'fact',\n",
       " 'factory',\n",
       " 'fairly',\n",
       " 'fake',\n",
       " 'family',\n",
       " 'fan',\n",
       " 'far',\n",
       " 'farm',\n",
       " 'farmer',\n",
       " 'farming',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'favorite',\n",
       " 'feed',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'fellow',\n",
       " 'felt',\n",
       " 'fiber',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'fine',\n",
       " 'fish',\n",
       " 'fit',\n",
       " 'flavor',\n",
       " 'flax',\n",
       " 'flour',\n",
       " 'focus',\n",
       " 'folk',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'food',\n",
       " 'form',\n",
       " 'format=pjpg',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'fridge',\n",
       " 'fried',\n",
       " 'friend',\n",
       " 'friendly',\n",
       " 'frozen',\n",
       " 'fruit',\n",
       " 'fry',\n",
       " 'fuck',\n",
       " 'fucking',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'future',\n",
       " 'gain',\n",
       " 'game',\n",
       " 'garlic',\n",
       " 'gas',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'genuinely',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'gluten',\n",
       " 'goal',\n",
       " 'god',\n",
       " 'going',\n",
       " 'gon',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'google',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'grain',\n",
       " 'gram',\n",
       " 'great',\n",
       " 'greatly',\n",
       " 'green',\n",
       " 'grocery',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'growing',\n",
       " 'gt',\n",
       " 'guess',\n",
       " 'guy',\n",
       " 'ha',\n",
       " 'habit',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happens',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'harm',\n",
       " 'hate',\n",
       " 'havenâ\\x80\\x99t',\n",
       " 'having',\n",
       " 'head',\n",
       " 'health',\n",
       " 'healthier',\n",
       " 'healthy',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'heavy',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'hey',\n",
       " 'heâ\\x80\\x99s',\n",
       " 'hi',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'hit',\n",
       " 'home',\n",
       " 'honestly',\n",
       " 'honey',\n",
       " 'hope',\n",
       " 'hopefully',\n",
       " 'hoping',\n",
       " 'horrible',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'house',\n",
       " 'http',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'hummus',\n",
       " 'hungry',\n",
       " 'hurt',\n",
       " 'husband',\n",
       " 'ice',\n",
       " 'idea',\n",
       " 'idk',\n",
       " 'im',\n",
       " 'imagine',\n",
       " 'impact',\n",
       " 'important',\n",
       " 'impossible',\n",
       " 'include',\n",
       " 'including',\n",
       " 'increase',\n",
       " 'individual',\n",
       " 'industry',\n",
       " 'info',\n",
       " 'information',\n",
       " 'ingredient',\n",
       " 'instead',\n",
       " 'intake',\n",
       " 'interested',\n",
       " 'interesting',\n",
       " 'iron',\n",
       " 'isnâ\\x80\\x99t',\n",
       " 'issue',\n",
       " 'item',\n",
       " 'itâ\\x80\\x99s',\n",
       " 'ive',\n",
       " 'iâ\\x80\\x99d',\n",
       " 'iâ\\x80\\x99ll',\n",
       " 'iâ\\x80\\x99m',\n",
       " 'iâ\\x80\\x99ve',\n",
       " 'job',\n",
       " 'journey',\n",
       " 'juice',\n",
       " 'junk',\n",
       " 'kale',\n",
       " 'keeping',\n",
       " 'kept',\n",
       " 'keto',\n",
       " 'kid',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'killing',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'kitchen',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'knowing',\n",
       " 'known',\n",
       " 'label',\n",
       " 'lack',\n",
       " 'land',\n",
       " 'large',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'lb',\n",
       " 'le',\n",
       " 'lead',\n",
       " 'learn',\n",
       " 'learned',\n",
       " 'learning',\n",
       " 'leather',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'legume',\n",
       " 'lentil',\n",
       " 'let',\n",
       " 'level',\n",
       " 'life',\n",
       " 'lifestyle',\n",
       " 'light',\n",
       " 'like',\n",
       " 'likely',\n",
       " 'line',\n",
       " 'link',\n",
       " 'list',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'livestock',\n",
       " 'living',\n",
       " 'local',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'lose',\n",
       " 'losing',\n",
       " 'loss',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'lt',\n",
       " 'lunch',\n",
       " 'main',\n",
       " 'mainly',\n",
       " 'major',\n",
       " 'majority',\n",
       " 'making',\n",
       " 'male',\n",
       " 'man',\n",
       " 'market',\n",
       " 'matter',\n",
       " 'maybe',\n",
       " 'meal',\n",
       " 'mean',\n",
       " 'meat',\n",
       " 'medical',\n",
       " 'medication',\n",
       " 'medium',\n",
       " 'meet',\n",
       " 'member',\n",
       " 'men',\n",
       " 'mental',\n",
       " 'mention',\n",
       " 'mentioned',\n",
       " 'menu',\n",
       " 'message',\n",
       " 'method',\n",
       " 'milk',\n",
       " 'million',\n",
       " 'mind',\n",
       " 'minute',\n",
       " 'miss',\n",
       " 'missing',\n",
       " 'mix',\n",
       " 'mixed',\n",
       " 'mom',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'month',\n",
       " 'moral',\n",
       " 'morning',\n",
       " 'mother',\n",
       " 'movement',\n",
       " 'moving',\n",
       " 'multiple',\n",
       " 'muscle',\n",
       " 'mushroom',\n",
       " 'na',\n",
       " 'natural',\n",
       " 'naturally',\n",
       " 'nature',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'new',\n",
       " 'nice',\n",
       " 'night',\n",
       " 'non',\n",
       " 'non-vegan',\n",
       " 'normal',\n",
       " 'normally',\n",
       " 'note',\n",
       " 'notice',\n",
       " 'noticed',\n",
       " 'number',\n",
       " 'nut',\n",
       " 'nutrient',\n",
       " 'nutrition',\n",
       " 'nutritional',\n",
       " 'oat',\n",
       " 'oatmeal',\n",
       " 'obviously',\n",
       " 'offer',\n",
       " 'oh',\n",
       " 'oil',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'olive',\n",
       " 'omega',\n",
       " 'omni',\n",
       " 'omnivore',\n",
       " 'onion',\n",
       " 'online',\n",
       " 'open',\n",
       " 'opinion',\n",
       " 'option',\n",
       " 'order',\n",
       " 'ordered',\n",
       " 'organic',\n",
       " 'outside',\n",
       " 'overall',\n",
       " 'page',\n",
       " 'pain',\n",
       " 'pan',\n",
       " 'parent',\n",
       " 'partner',\n",
       " 'past',\n",
       " 'pasta',\n",
       " 'pay',\n",
       " 'pea',\n",
       " 'peanut',\n",
       " 'people',\n",
       " 'pepper',\n",
       " 'perfect',\n",
       " 'period',\n",
       " 'person',\n",
       " 'personal',\n",
       " 'personally',\n",
       " 'perspective',\n",
       " 'pet',\n",
       " 'picture',\n",
       " 'piece',\n",
       " 'pig',\n",
       " 'pizza',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planet',\n",
       " 'planning',\n",
       " 'plant',\n",
       " 'plant-based',\n",
       " 'plenty',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'poll',\n",
       " 'poor',\n",
       " 'population',\n",
       " 'pork',\n",
       " 'positive',\n",
       " 'possible',\n",
       " 'possibly',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'pot',\n",
       " 'potato',\n",
       " 'pound',\n",
       " 'powder',\n",
       " 'practice',\n",
       " 'prefer',\n",
       " 'prep',\n",
       " 'pressure',\n",
       " 'pretty',\n",
       " 'price',\n",
       " 'probably',\n",
       " 'problem',\n",
       " 'process',\n",
       " 'processed',\n",
       " 'produce',\n",
       " 'product',\n",
       " 'production',\n",
       " 'project',\n",
       " 'protein',\n",
       " 'provide',\n",
       " 'purpose',\n",
       " 'putting',\n",
       " 'quality',\n",
       " 'question',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quinoa',\n",
       " 'range',\n",
       " 'raw',\n",
       " 'read',\n",
       " 'reading',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'realize',\n",
       " 'realized',\n",
       " 'reason',\n",
       " 'recently',\n",
       " 'recipe',\n",
       " 'recommend',\n",
       " 'recommendation',\n",
       " 'recommended',\n",
       " 'red',\n",
       " 'reddit',\n",
       " 'reduce',\n",
       " 'regular',\n",
       " 'regularly',\n",
       " 'related',\n",
       " 'relationship',\n",
       " 'remember',\n",
       " 'replace',\n",
       " 'replacement',\n",
       " 'research',\n",
       " 'resource',\n",
       " 'respect',\n",
       " 'response',\n",
       " 'rest',\n",
       " 'restaurant',\n",
       " 'result',\n",
       " 'rice',\n",
       " 'right',\n",
       " 'risk',\n",
       " 'room',\n",
       " 'run',\n",
       " 'running',\n",
       " 'sad',\n",
       " 'safe',\n",
       " 'said',\n",
       " 'salad',\n",
       " 'salt',\n",
       " 'sanctuary',\n",
       " 'sandwich',\n",
       " 'sauce',\n",
       " 'save',\n",
       " 'saw',\n",
       " 'saying',\n",
       " 'school',\n",
       " 'science',\n",
       " 'second',\n",
       " 'seed',\n",
       " 'seeing',\n",
       " 'seen',\n",
       " 'seitan',\n",
       " 'sell',\n",
       " 'sense',\n",
       " 'seriously',\n",
       " 'serving',\n",
       " 'set',\n",
       " 'share',\n",
       " 'shit',\n",
       " 'shoe',\n",
       " 'shop',\n",
       " 'shopping',\n",
       " 'short',\n",
       " 'sick',\n",
       " 'similar',\n",
       " 'simple',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'sister',\n",
       " 'site',\n",
       " 'situation',\n",
       " 'skin',\n",
       " 'slaughter',\n",
       " 'sleep',\n",
       " 'slowly',\n",
       " 'small',\n",
       " 'smell',\n",
       " 'smoothie',\n",
       " 'smoothy',\n",
       " 'snack',\n",
       " 'social',\n",
       " 'society',\n",
       " 'solution',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'sort',\n",
       " 'sound',\n",
       " 'soup',\n",
       " 'source',\n",
       " 'soy',\n",
       " 'space',\n",
       " 'specie',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'spend',\n",
       " 'spice',\n",
       " 'spinach',\n",
       " 'spread',\n",
       " 'stand',\n",
       " 'standard',\n",
       " 'start',\n",
       " 'started',\n",
       " 'starting',\n",
       " 'state',\n",
       " 'stay',\n",
       " 'steak',\n",
       " 'step',\n",
       " 'stick',\n",
       " 'stomach',\n",
       " 'stop',\n",
       " 'stopped',\n",
       " 'store',\n",
       " 'story',\n",
       " 'straight',\n",
       " 'strong',\n",
       " 'struggle',\n",
       " 'struggling',\n",
       " 'student',\n",
       " 'study',\n",
       " 'stuff',\n",
       " 'stupid',\n",
       " 'sub',\n",
       " 'subreddit',\n",
       " 'substitute',\n",
       " 'suffering',\n",
       " 'sugar',\n",
       " 'suggest',\n",
       " 'suggestion',\n",
       " 'super',\n",
       " 'supplement',\n",
       " 'support',\n",
       " 'supposed',\n",
       " 'sure',\n",
       " 'survey',\n",
       " 'sustainable',\n",
       " 'sweet',\n",
       " 'switch',\n",
       " 'switched',\n",
       " 'symptom',\n",
       " 'system',\n",
       " 'table',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'taste',\n",
       " 'tasty',\n",
       " 'tbsp',\n",
       " 'tea',\n",
       " 'tell',\n",
       " 'telling',\n",
       " 'tend',\n",
       " 'term',\n",
       " 'terrible',\n",
       " 'test',\n",
       " 'texture',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thatâ\\x80\\x99s',\n",
       " 'thereâ\\x80\\x99s',\n",
       " 'theyâ\\x80\\x99re',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thinking',\n",
       " 'thought',\n",
       " 'throw',\n",
       " 'time',\n",
       " 'tip',\n",
       " 'tired',\n",
       " 'title',\n",
       " 'today',\n",
       " 'tofu',\n",
       " 'told',\n",
       " 'tomato',\n",
       " 'ton',\n",
       " 'took',\n",
       " 'topic',\n",
       " 'total',\n",
       " 'totally',\n",
       " 'transition',\n",
       " 'transitioning',\n",
       " 'treat',\n",
       " 'tried',\n",
       " 'trip',\n",
       " 'trouble',\n",
       " 'true',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " 'turn',\n",
       " 'turned',\n",
       " 'type',\n",
       " 'u',\n",
       " 'uk',\n",
       " 'understand',\n",
       " 'unfortunately',\n",
       " 'unhealthy',\n",
       " 'upset',\n",
       " 'use',\n",
       " 'usually',\n",
       " 'value',\n",
       " 'variety',\n",
       " 'veg',\n",
       " 'vegan',\n",
       " 'veganism',\n",
       " 'vegetable',\n",
       " 'vegetarian',\n",
       " 'veggie',\n",
       " 'version',\n",
       " 'video',\n",
       " 'view',\n",
       " 'vitamin',\n",
       " 'wa',\n",
       " 'wan',\n",
       " 'want',\n",
       " 'wanted',\n",
       " 'wanting',\n",
       " 'wasnâ\\x80\\x99t',\n",
       " 'waste',\n",
       " 'watch',\n",
       " 'watched',\n",
       " 'watching',\n",
       " 'water',\n",
       " 'way',\n",
       " 'website',\n",
       " 'week',\n",
       " 'weekend',\n",
       " 'weight',\n",
       " 'weird',\n",
       " 'welcome',\n",
       " 'went',\n",
       " 'weâ\\x80\\x99re',\n",
       " 'wfpb',\n",
       " 'whatâ\\x80\\x99s',\n",
       " 'wheat',\n",
       " 'white',\n",
       " 'wife',\n",
       " 'wild',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'wo',\n",
       " 'woman',\n",
       " 'wonder',\n",
       " 'wondering',\n",
       " 'wonâ\\x80\\x99t',\n",
       " 'word',\n",
       " 'work',\n",
       " 'worked',\n",
       " 'working',\n",
       " 'world',\n",
       " 'worried',\n",
       " 'worry',\n",
       " 'worse',\n",
       " 'worth',\n",
       " 'wrong',\n",
       " 'x200b',\n",
       " 'yeah',\n",
       " 'year',\n",
       " 'yeast',\n",
       " 'yes',\n",
       " 'yesterday',\n",
       " 'yogurt',\n",
       " 'young',\n",
       " 'youtube',\n",
       " 'youâ\\x80\\x99re',\n",
       " '|',\n",
       " '»',\n",
       " 'â',\n",
       " 'â\\x80\\x9d']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cite source\n",
    "[regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9082e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38b38238-b208-42ee-b3f8-02825f195b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28940\\3826876641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                  n_jobs = -1)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1097\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 975\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    976\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    566\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    439\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(xg.XGBClassifier(),\n",
    "                  param_grid = {\n",
    "                      'n_estimators':[30,50,100,200]},\n",
    "                  cv=5,\n",
    "                 verbose=1,\n",
    "                 n_jobs = -1)\n",
    "gs.fit(X_train, y_train)\n",
    "xgb = gs.best_estimator_.fit\n",
    "xgb.fit(X_train,y_train)\n",
    "xgb.score(X_train, y_train), xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc82bb5",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b539d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b96898",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = StackingClassifier(estimators = [('logr',logr),\n",
    "                                         ('mnb',mnb),\n",
    "                                         ('knn',knn),\n",
    "                                         ('dt',dt)\n",
    "                                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ce25a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stack.fit(X_train,y_train)\n",
    "stack.score(X_train,y_train),stack.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b5177-1c7a-4445-ae03-9435a5b64ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cc2b0-f798-4fa4-bad8-b042d7a140e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c30c943-7e3f-452c-ae19-f0d8afaeef40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
